{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "compute log prob increase, and analyze the attention of the last token'Q and the context's K\n",
    "'''\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from baukit import TraceDict\n",
    "import copy\n",
    "\n",
    "device=\"cuda:1\"\n",
    "model_name='gpt-xl'\n",
    "model_path_dict={\n",
    "    'llama':\"/home/pc/data/qixun/checkpoints/llama-2-7b\",\n",
    "    'gpt-xl':\"/home/pc/data/qixun/checkpoints/gpt2-xl\"\n",
    "}\n",
    "model_path=model_path_dict[model_name]\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,torch_dtype=torch.float16).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pc/data/qixun/codes/Quick_LLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/data/qixun/conda_envs/fv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/pc/data/qixun/codes/Quick_LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM\n",
    "from networks.my_llama_attn_block import CustomLlamaAttention\n",
    "from networks.my_gpt2_attn_block import CustomGPT2Attention\n",
    "from dataset import ICLDataLoader\n",
    "import random\n",
    "\n",
    "MAX_NEW_TOKENS=2\n",
    "n_context=5\n",
    "dataset_size=500\n",
    "n_layers=model.config.num_hidden_layers\n",
    "n_heads=model.config.num_attention_heads\n",
    "hidden_dim=model.config.hidden_size\n",
    "head_dim=hidden_dim//n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_log_pred_prob(target_word, model, tokenizer, rep, word_embeddings):\n",
    "    assert rep.shape == torch.Size([model.config.hidden_size])\n",
    "    target_token=tokenizer(target_word, return_tensors='pt')['input_ids'].squeeze().to(device)\n",
    "    assert target_token.shape[0]==2 # some target words may longer than 1 token\n",
    "    target_token=target_token[1] # target_token[0] is <s>\n",
    "    target_word_embedding=word_embeddings[target_token]\n",
    "    sum_exp=torch.sum(torch.exp(torch.sum(word_embeddings * rep, dim=1)), dim=0)\n",
    "    target_exp=torch.exp(torch.dot(target_word_embedding, rep))\n",
    "    log_prob=torch.log(target_exp/sum_exp)\n",
    "    return log_prob\n",
    "\n",
    "def get_output_text_form_prompt(model, tokenizer, prompt, device):\n",
    "    input_tks = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    output_tokens = model.generate(input_tks.input_ids, top_p=0.9, temperature=0.1,\n",
    "                                max_new_tokens=MAX_NEW_TOKENS)\n",
    "    output_text = tokenizer.decode(output_tokens.squeeze()[-MAX_NEW_TOKENS:])\n",
    "    return output_text\n",
    "\n",
    "def get_output_text_form_logits(logits):\n",
    "    output_tokens = torch.topk(logits, k=1).indices.squeeze(0)\n",
    "    output_text = tokenizer.decode(output_tokens)\n",
    "    return output_text\n",
    "\n",
    "def get_tokens(model, tokenizer, prompt, device):\n",
    "    tokens = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    return tokens['input_ids']\n",
    "\n",
    "def get_tk_words(tokenizer, tks):\n",
    "    '''\n",
    "    map the tokens back to the words. original words may be split.\n",
    "    '''\n",
    "    tk_words=[]\n",
    "    for i in range(tks.shape[0]):\n",
    "        tk_words.append(tokenizer.decode(tks[i]))\n",
    "    return tk_words\n",
    "\n",
    "def jaccard(a,b):\n",
    "    set_a=set(a)\n",
    "    set_b = set(b)\n",
    "    return len(set_a.intersection(set_b))/len(set_a.union(set_b))\n",
    "\n",
    "def block_head(edit_layer, head_to_block):\n",
    "    def block_head_(output, layer_name):\n",
    "        current_layer = int(layer_name.split(\".\")[2])\n",
    "        if current_layer == edit_layer: # 遍历到edit_layer, 该修改了\n",
    "            if isinstance(output, tuple):\n",
    "                # output[0].shape [1,12,4096] [1, num_token, dim_hidden]\n",
    "                if head_to_block==-1:\n",
    "                    output[0][:, :, :] = 0 # block all heads\n",
    "                else:\n",
    "                    output[0][:, :, head_to_block*head_dim:(head_to_block+1):head_dim] = 0 # block a certain head output\n",
    "                return output\n",
    "            else:\n",
    "                if head_to_block==-1:\n",
    "                    output[:, :, :] = 0 # block all heads\n",
    "                else:\n",
    "                    output[:, :, head_to_block*head_dim:(head_to_block+1):head_dim] = 0 # block a certain head output\n",
    "                return output\n",
    "        else:\n",
    "            return output\n",
    "    return block_head_\n",
    "\n",
    "\n",
    "def visualize_bar(x, y, title_name, x_name, y_name):\n",
    "    plt.bar(x, y.detach().cpu().numpy(), color='skyblue')\n",
    "    plt.title(title_name)\n",
    "    plt.xlabel(x_name)\n",
    "    plt.ylabel(y_name)\n",
    "    plt.grid(True)\n",
    "\n",
    "def visualize_heatmap(data, title_name, x_name, y_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(data, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar()  # 显示颜色条\n",
    "    plt.title(title_name)\n",
    "    plt.xlabel(x_name)\n",
    "    plt.ylabel(y_name)\n",
    "    \n",
    "    \n",
    "def replace_attn(model_name, model, layers, block_pos1, block_pos2):\n",
    "    original_self_attns=[]\n",
    "    for l in layers:\n",
    "        if model_name=='gpt-xl':\n",
    "            original_self_attn = model.transformer.h[l].attn\n",
    "            original_self_attns.append(original_self_attn)\n",
    "            original_weights = {name: param.clone() for name, param in original_self_attn.named_parameters()} \n",
    "            custom_attn = CustomGPT2Attention(model.config, layer_idx=l).half().to(device)\n",
    "            with torch.no_grad():\n",
    "                for name, param in custom_attn.named_parameters():\n",
    "                    if name not in ['block_pos1', 'block_pos2']:\n",
    "                        param.copy_(original_weights[name])\n",
    "            model.transformer.h[l].attn = custom_attn\n",
    "            model.transformer.h[l].attn.set_block_positions(block_pos1, block_pos2)\n",
    "        elif model_name=='llama':\n",
    "            original_self_attn = model.model.layers[l].self_attn\n",
    "            original_weights = {name: param.clone() for name, param in original_self_attn.named_parameters()}\n",
    "            custom_attn = CustomLlamaAttention(model.config).half().to(device)# .half() is torch.float16\n",
    "            # 加载权重到 CustomLlamaAttention\n",
    "            with torch.no_grad():\n",
    "                for name, param in custom_attn.named_parameters():\n",
    "                    if name not in ['block_pos1', 'block_pos2']:\n",
    "                        param.copy_(original_weights[name])\n",
    "            model.model.layers[l].self_attn = custom_attn\n",
    "            model.model.layers[l].self_attn.set_block_positions(block_pos1, block_pos2) \n",
    "            #print(original_self_attn) #right, it's LlamaAttention here, not CustomLlamaAttention\n",
    "\n",
    "    return original_self_attns\n",
    "    \n",
    "def recover_attn(model_name, model, layers, original_self_attns, interval_len):\n",
    "    #print(layers)\n",
    "    for i in range(interval_len):\n",
    "        if model_name=='gpt-xl':\n",
    "            model.transformer.h[layers[i]].attn = original_self_attns[i]\n",
    "        elif model_name=='llama':\n",
    "            model.model.layers[layers[i]].self_attn = original_self_attns[i]\n",
    "            \n",
    "            \n",
    "def find_subsequence(tensor, subsequence):\n",
    "    \"\"\"\n",
    "    在1D Tensor中查找子序列的位置.\n",
    "    Args:\n",
    "        tensor (torch.Tensor): 目标1D Tensor.\n",
    "        subsequence (torch.Tensor): 要查找的子序列.\n",
    "    Returns:\n",
    "        int: 子序列起始位置的索引，如果未找到则返回-1.\n",
    "    \"\"\"\n",
    "    # 获取子序列的长度\n",
    "    sub_len = len(subsequence)\n",
    "    res=[]\n",
    "    for i in range(len(tensor) - sub_len + 1):\n",
    "        # 检查是否匹配\n",
    "        if torch.equal(tensor[i:i + sub_len], subsequence):\n",
    "            res.append(i)\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evalutating:   0%|          | 1/1000 [04:49<80:25:38, 289.83s/it]\n",
      "Evalutating:   0%|          | 1/1000 [00:01<32:39,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Sentence: can be considered work  Answer: 1\n",
      "Sentence: many flaws  Answer: 0\n",
      "Sentence: cable channel  Answer: 0\n",
      "Sentence: otherwise dull subjects  Answer: 0\n",
      "Sentence: there is nothing redeeming about this movie .  Answer: 0\n",
      "Sentence: hide new secretions from the parental units  Answer: \n",
      "tensor([31837,   594,    25,   460,   307,  3177,   670,   220, 23998,    25,\n",
      "          352,   198, 31837,   594,    25,   867, 17978,   220, 23998,    25,\n",
      "          657,   198, 31837,   594,    25,  7862,  6518,   220, 23998,    25,\n",
      "          657,   198, 31837,   594,    25,  4306, 19222,  7481,   220, 23998,\n",
      "           25,   657,   198, 31837,   594,    25,   612,   318,  2147, 26509,\n",
      "          278,   546,   428,  3807,   764,   220, 23998,    25,   657,   198,\n",
      "        31837,   594,    25,  7808,   649,  3200,   507,   422,   262, 21694,\n",
      "         4991,   220, 23998,    25,   220], device='cuda:1')\n",
      "tensor([11, 21, 31, 42, 59], device='cuda:1')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 131\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(sep_poss)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#print(colon_pos)\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m x_poss\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep_poss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolon_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#print(x_poss)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m block_pos_dict\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrand1\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, input_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mlong(),\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrand2\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, input_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mlong(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m:torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, input_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    142\u001b[0m }\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "'''\n",
    "compute the log probability increase of the layers: log p(w| VO ) - log p(w| before O)\n",
    "or the heads: log p(w| V[h] @ O [h]) - log p(w| before O)\n",
    "'''\n",
    "task='sst-2' # which prompt to test\n",
    "spec_str='' # flexible str to mark the saved figures\n",
    "ana_module='spec_layer' # all heads: compute the attn of all heads; by_head: compute the attn of the most significant heads; \n",
    "# spec_head: heatmap of attn diff of all heads; spec_layer: visualize the attn of a specified layer\n",
    "#print(model)\n",
    "if task=='a':\n",
    "    task_name=\"antonym\"\n",
    "    data_path=\"/home/pc/data/qixun/datasets/NLP_datasets/filtered_antonym100_single_token.json\"\n",
    "elif task=='ss':\n",
    "    task_name=\"simple_sentiment\"\n",
    "    data_path=\"/home/pc/data/qixun/datasets/NLP_datasets/sentiment100.json\"\n",
    "elif task=='sss':\n",
    "    task_name=\"simple_sentiment_symbol\"\n",
    "    data_path=\"/home/pc/data/qixun/datasets/NLP_datasets/sentiment100_symbol.json\"\n",
    "    spec_str+='_foo'\n",
    "    target_word=\"foo\"\n",
    "elif task=='sst-2':\n",
    "    task_name='sst-2'\n",
    "    data_path='/home/pc/data/qixun/datasets/NLP_datasets/sst2'\n",
    "elif task=='ss2':\n",
    "    task_name=\"simple_sentiment_2_words\"\n",
    "    prompt=\"revolutionary idea: positive\\nbad ass: negative\\ninsightful finding: positive\\nselfish man: negative\\nbeautiful bird: positive\\noutdated book: negative\\nfilthy idea: \"\n",
    "    target_word=\"negative\"\n",
    "elif task=='s_adv1':\n",
    "    task_name=\"sentiment_adv1\"\n",
    "    prompt=\"Q: A revolutionary device that sets a new standard in its category with its performance and design, jeans, flamingo, xylophone chinchilla and mandrill. A: t\\\n",
    "    \\nQ: Overcrowded and overpriced, this place lacks the charm it promises in brochures. A: z\\\n",
    "    \\nQ: Engaging topics and insightful hosts make for a compelling listen every week. A: t\\\n",
    "    \\nQ: Constant buffering and low resolution ruin what could have been an enjoyable movie night. A: z\\\n",
    "    \\nQ: Intuitive design and rich features make this tool indispensable for professionals in the field. A: t\\\n",
    "    \\nQ: The course content was outdated, and the instructor interaction was minimal. A: z\\\n",
    "    \\nQ: The blender is extremely loud and difficult to clean after use, jeans, flamingo, xylophone chinchilla and mandrill. A:\"\n",
    "    target_word=\"t\"\n",
    "elif task=='s_adv2':\n",
    "    task_name=\"sentiment_adv2\"\n",
    "    prompt=\"Q: A revolutionary device that sets a new standard in its category with its performance and design. A: t\\\n",
    "    \\nQ: Overcrowded and overpriced, this place lacks the charm it promises in brochures. A: z\\\n",
    "    \\nQ: Engaging topics and insightful hosts make for a compelling listen every week, jeans, flamingo, xylophone chinchilla and mandrill. A: t\\\n",
    "    \\nQ: Constant buffering and low resolution ruin what could have been an enjoyable movie night. A: z\\\n",
    "    \\nQ: Intuitive design and rich features make this tool indispensable for professionals in the field. A: t\\\n",
    "    \\nQ: The course content was outdated, and the instructor interaction was minimal. A: z\\\n",
    "    \\nQ: The blender is extremely loud and difficult to clean after use, jeans, flamingo, xylophone chinchilla and mandrill. A:\"\n",
    "    target_word=\"t\"\n",
    "elif task=='s_adv3':\n",
    "    task_name=\"sentiment_adv3\"\n",
    "    prompt=\"Q: A revolutionary device that sets a new standard in its category with its performance and design. A: t\\\n",
    "    \\nQ: Overcrowded and overpriced, this place lacks the charm it promises in brochures. A: z\\\n",
    "    \\nQ: Engaging topics and insightful hosts make for a compelling listen every week. A: t\\\n",
    "    \\nQ: Constant buffering and low resolution ruin what could have been an enjoyable movie night. A: z\\\n",
    "    \\nQ: Intuitive design and rich features make this tool indispensable for professionals in the field, jeans, flamingo, xylophone chinchilla and mandrill. A: t\\\n",
    "    \\nQ: The course content was outdated, and the instructor interaction was minimal. A: z\\\n",
    "    \\nQ: The blender is extremely loud and difficult to clean after use, jeans, flamingo, xylophone chinchilla and mandrill. A:\"\n",
    "    print(get_tokens(model, tokenizer, prompt, device))\n",
    "    target_word=\"t\"\n",
    "elif task=='s':\n",
    "    task_name=\"sentiment\"\n",
    "    prompt=\"Q: A revolutionary device that sets a new standard in its category with its performance and design. A: t\\\n",
    "    \\nQ: Overcrowded and overpriced, this place lacks the charm it promises in brochures. A: z\\\n",
    "    \\nQ: Engaging topics and insightful hosts make for a compelling listen every week. A: t\\\n",
    "    \\nQ: Constant buffering and low resolution ruin what could have been an enjoyable movie night. A: z\\\n",
    "    \\nQ: Intuitive design and rich features make this tool indispensable for professionals in the field. A: t\\\n",
    "    \\nQ: The course content was outdated, and the instructor interaction was minimal. A: z\\\n",
    "    \\nQ: The blender is extremely loud and difficult to clean after use. A:\"\n",
    "    target_word=\"bar\"\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "dataset=ICLDataLoader(file_path=data_path, context_size=n_context, dataset_size=dataset_size)\n",
    "\n",
    "attn_O=[f'model.layers.{l}.self_attn.o_proj' for l in range(n_layers)]\n",
    "attn_V=[f'model.layers.{l}.self_attn.v_proj' for l in range(n_layers)]\n",
    "attn_Q=[f'model.layers.{l}.self_attn.q_proj' for l in range(n_layers)]\n",
    "attn_K=[f'model.layers.{l}.self_attn.k_proj' for l in range(n_layers)]\n",
    "n_top_preds=5 # numbers of top predicted tokens for jaccard similarity computation\n",
    "block_obj='block_spec' # block attention from block_obj2 to block_obj1\n",
    "block_layers='some_layers' # \n",
    "assert block_layers in ['some_layers', 'all_layers']\n",
    "\n",
    "if task_name in ['a', 'ss', 'sss']:\n",
    "    block_obj1_list=['lst', 'lb','sep']\n",
    "    block_obj2_list=['x','lb','sep']\n",
    "elif task_name in ['sst-2']:\n",
    "    block_obj1_list=['lst', 'lst_x', 'lb','sep1', 'sep2']\n",
    "    block_obj2_list=['x','lb','sep1', 'sep2']\n",
    "\n",
    "#block_obj1_list=['all']\n",
    "#block_obj2_list=['all']\n",
    "\n",
    "# print(word_embeddings.weight.shape) # torch.Size([32000, 4096])\n",
    "word_embeddings=model.get_input_embeddings().weight\n",
    "\n",
    "#block_top_tks=torch.zeros(n_layers, n_heads, n_top_preds).to(device)\n",
    "\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "for block_obj1 in block_obj1_list:\n",
    "    for block_obj2 in block_obj2_list:\n",
    "        spec_str=''\n",
    "        all_jaccard_scores=[]\n",
    "        for i, prompt in enumerate(dataset):\n",
    "            print(\"prompt:\", prompt)\n",
    "            input_tokens = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "            #print(\"model output:\",get_output_text_form_logits(model(**input_tokens).logits[:, -1]))\n",
    "            #with TraceDict(model, layers=attn_O+attn_Q+attn_V+attn_K, retain_input=True, retain_output=True) as _:\n",
    "            #print(input_tokens)\\\n",
    "            if model_name=='gpt-xl':\n",
    "                sep_tk=tokenizer('\\n', return_tensors='pt').to(device)['input_ids'][0] # separator between examples\n",
    "            elif model_name=='llama':\n",
    "                colon_tk=29901\n",
    "                sep_tk=13\n",
    "            label_poss=torch.where(input_tokens['input_ids'][0]==sep_tk)[0]-1  # find '\\n', and use its previous token\n",
    "            sep_poss=torch.where(input_tokens['input_ids'][0]==sep_tk)[0]\n",
    "            last_pos=[int(len(input_tokens['input_ids'][0])-1)]\n",
    "            colon_pos=torch.where(input_tokens['input_ids'][0]==colon_tk)[0]\n",
    "            if model_name=='gpt-xl':\n",
    "                if task_name=='sst-2':\n",
    "                    sep1_tks=torch.tensor([31837, 594, 25]) #Sentence: \n",
    "                    sep2_tks=torch.tensor([220, 23998, 25]) #Answer:\n",
    "                    all_sep1_poss=find_subsequence(input_tokens['input_ids'][0], sep1_tks)\n",
    "                    sep1_poss=all_sep1_poss[:-2]\n",
    "                    lst_sep1_pos=all_sep1_poss[-1]\n",
    "                    all_sep2_poss=find_subsequence(input_tokens['input_ids'][0], sep2_tks)\n",
    "                    sep2_poss=all_sep2_poss[:-2]\n",
    "                    lst_sep2_pos=all_sep2_poss[-1]\n",
    "                    lst_x_poss=torch.arange(lst_sep1_pos+3, lst_sep2_pos) # query x\n",
    "                    x_poss=[e for e in torch.arange(input_tokens['input_ids'][0].shape[0]).tolist() \\\n",
    "                        if e not in all_sep1_poss.tolist()+all_sep2_poss.tolist()+sep_poss.tolist()+label_poss.tolist()+lst_x_poss.tolist()+last_pos]\n",
    "                    colon_tk=tokenizer(':',return_tensors='pt').to(device)['input_ids'][0] #  \n",
    "                elif task_name in ['a','ss','sss']:\n",
    "                    x_poss=[]\n",
    "            elif model_name=='llama':\n",
    "                if task_name=='sst-2':\n",
    "                    raise NotImplementedError\n",
    "            #print(colon_tk, sep_tk)\n",
    "            \n",
    "            \n",
    "            #print([torch.range(s,e) for s,e in enumerate(torch.cat((torch.tensor(0).to(device), sep_poss), dim=0)+1, colon_pos)])\n",
    "            #print(zip(torch.cat((torch.tensor([0]).to(device), sep_poss), dim=0), colon_pos))\n",
    "            print(input_tokens['input_ids'][0])\n",
    "            print(sep_poss)\n",
    "            #print(colon_pos)\n",
    "            #x_poss=torch.cat([torch.arange(s+1,e) for s,e in zip(torch.cat((torch.tensor([0]).to(device), sep_poss), dim=0), colon_pos)], dim=0)\n",
    "            #print(x_poss)\n",
    "            block_pos_dict={\n",
    "                'rand1': torch.from_numpy(np.random.choice(np.arange(0, input_tokens['input_ids'][0].shape[0]), 1)).long(),\n",
    "                'rand2': torch.from_numpy(np.random.choice(np.arange(0, input_tokens['input_ids'][0].shape[0]), 1)).long(),\n",
    "                'lb': label_poss.long(),\n",
    "                'sep': sep_poss.long(),\n",
    "                'sep1': all_sep1_poss.long(),\n",
    "                'sep2': all_sep2_poss.long(),\n",
    "                'lst': torch.tensor(last_pos).long(),\n",
    "                'lst_x': lst_x_poss.long(), # query x, only imple\n",
    "                'x': x_poss.long(), # x of demos\n",
    "                'empty':torch.tensor([]).long(),\n",
    "                'all':torch.arange(0, input_tokens['input_ids'][0].shape[0]).long()\n",
    "            }\n",
    "            block_pos1=block_pos_dict[block_obj1]\n",
    "            block_pos2=block_pos_dict[block_obj2]\n",
    "            len_block_pos1=block_pos1.shape[0]\n",
    "            len_block_pos2=block_pos2.shape[0]\n",
    "            block_pos1=block_pos1.unsqueeze(1).repeat(1,len_block_pos2).view(-1) # [1,...,1,4,...,4,8,...,8] each element of ori block_pos1 repeat block_pos2.shape[0] times\n",
    "            block_pos2=block_pos2.repeat(1, len_block_pos1)\n",
    "            #print(\"1:\", block_pos1, \"2:\",block_pos2)\n",
    "            \n",
    "            #tensor([    1,   302,  4227,  3864, 29901,  6374,    13,  7665,  5414, 29901,\n",
    "            # 8178,    13, 29888,   424,  6288, 29901,  6374,    13,   370,   375,\n",
    "            #  573, 29901,  8178,    13, 29920, 14596, 29901,  6374,    13,   314,\n",
    "            #  834,   292, 29901, 29871], device='cuda:0')   \n",
    "            \n",
    "            ''' sst-2\n",
    "            tensor([31837,   594,    25,   460,   307,  3177,   670,   220, 23998,    25,\n",
    "            352,   198, 31837,   594,    25,   867, 17978,   220, 23998,    25,\n",
    "            657,   198, 31837,   594,    25,  7862,  6518,   220, 23998,    25,\n",
    "            657,   198, 31837,   594,    25,  4306, 19222,  7481,   220, 23998,\n",
    "            25,   657,   198, 31837,   594,    25,   612,   318,  2147, 26509,\n",
    "            278,   546,   428,  3807,   764,   220, 23998,    25,   657,   198,\n",
    "            31837,   594,    25,  7808,   649,  3200,   507,   422,   262, 21694,\n",
    "            4991,   220, 23998,    25,   220], device='cuda:1')\n",
    "            '''\n",
    "               \n",
    "            #print(label_poss)\n",
    "            pred=model(**input_tokens).logits[:, -1]\n",
    "            clean_top_tks=torch.topk(pred, n_top_preds).indices.squeeze(0)\n",
    "            \n",
    "            if block_layers=='all_layers':\n",
    "                jaccard_scores_block=torch.zeros(n_layers).to(device)\n",
    "                for l in range(n_layers):\n",
    "                    #print(l)\n",
    "                    # tem_attn=copy.deepcopy(model.model.layers[l].self_attn)\n",
    "                    if model_name=='llama':\n",
    "                        original_self_attn = model.model.layers[l].self_attn\n",
    "                        original_weights = {name: param.clone() for name, param in original_self_attn.named_parameters()}\n",
    "                        custom_attn = CustomLlamaAttention(model.config).half().to(device)# .half() is torch.float16\n",
    "                        # 加载权重到 CustomLlamaAttention\n",
    "                        with torch.no_grad():\n",
    "                            for name, param in custom_attn.named_parameters():\n",
    "                                if name not in ['block_pos1', 'block_pos2']:\n",
    "                                    param.copy_(original_weights[name])\n",
    "                        model.model.layers[l].self_attn = custom_attn\n",
    "                        model.model.layers[l].self_attn.set_block_positions(block_pos1, block_pos2)            \n",
    "                        block_pred = model(**input_tokens).logits[0, -1]\n",
    "                        #print(original_self_attn) #right, it's LlamaAttention here, not CustomLlamaAttention\n",
    "                        model.model.layers[l].self_attn=original_self_attn\n",
    "                    elif model_name=='gpt-xl':\n",
    "                        original_self_attn = model.transformer.h[l].attn\n",
    "                        original_weights = {name: param.clone() for name, param in original_self_attn.named_parameters()} \n",
    "                        custom_attn = CustomGPT2Attention(model.config, layer_idx=l).half().to(device)\n",
    "                        with torch.no_grad():\n",
    "                            for name, param in custom_attn.named_parameters():\n",
    "                                if name not in ['block_pos1', 'block_pos2']:\n",
    "                                    param.copy_(original_weights[name])\n",
    "                        model.transformer.h[l].attn = custom_attn\n",
    "                        model.transformer.h[l].attn.set_block_positions(block_pos1, block_pos2)\n",
    "                        block_pred = model(**input_tokens).logits[0, -1]\n",
    "                        model.transformer.h[l].attn = original_self_attn\n",
    "                    jaccard_scores_block[l] = jaccard(clean_top_tks.tolist(), torch.topk(block_pred, n_top_preds).indices.tolist())\n",
    "                save_dir=f'./visualization/loyalty/block_spec/'+model_name\n",
    "                #print(jaccard_scores_block_lb-lst)\n",
    "                all_jaccard_scores.append(jaccard_scores_block.unsqueeze(0))  \n",
    "                x=range(n_layers)     \n",
    "            elif block_layers=='some_layers':\n",
    "                block_layerss=[]\n",
    "                interval_len=10\n",
    "                i=0\n",
    "                while i+interval_len-1<n_layers:\n",
    "                    block_layerss.append(range(i,i+interval_len)) \n",
    "                    i+=interval_len\n",
    "                block_layerss.append(range(n_layers-interval_len, n_layers))\n",
    "                #print(block_layerss)\n",
    "                jaccard_scores_block=torch.zeros(len(block_layerss)).to(device)\n",
    "                for i, layers in enumerate(block_layerss):\n",
    "                    original_self_attns=replace_attn(model_name, model, layers, block_pos1, block_pos2)\n",
    "                    block_pred = model(**input_tokens).logits[0, -1]\n",
    "                    recover_attn(model_name, model, layers, original_self_attns, interval_len)\n",
    "                    jaccard_scores_block[i] = jaccard(clean_top_tks.tolist(), torch.topk(block_pred, n_top_preds).indices.tolist())\n",
    "                save_dir=f'./visualization/loyalty/block_spec/'+model_name+'/block_multiple_layers'\n",
    "                all_jaccard_scores.append(jaccard_scores_block.unsqueeze(0))\n",
    "                x=[]\n",
    "                for layers in block_layerss:\n",
    "                    x.append(f'[{layers[0]},{layers[-1]}]')\n",
    "        \n",
    "        dataset_mean_loyalty=torch.mean(torch.cat(all_jaccard_scores,dim=0), dim=0)\n",
    "        spec_str+='_'+block_obj2+'-'+block_obj1\n",
    "        if block_layers=='some_layers':\n",
    "            spec_str += '_interval-'+str(interval_len)\n",
    "        visualize_bar(x=x,  y=dataset_mean_loyalty, title_name=task_name+spec_str, x_name='layers', y_name='loyalty')\n",
    "        spec_str+=f'_top{n_top_preds}tks_dataset-size-{dataset_size}_{n_context}-demos'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(save_dir, task_name+spec_str+'.jpg'))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Example, Token ID: 16281\n",
      "Token: Ġsentence, Token ID: 6827\n",
      "Token: :, Token ID: 25\n",
      "Token: Ġthis, Token ID: 428\n",
      "Token: Ġis, Token ID: 318\n",
      "Token: Ġa, Token ID: 257\n",
      "Token: Ġtest, Token ID: 1332\n",
      "Token: ., Token ID: 13\n",
      "[':'] [25]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Example sentence: this is a test.\"\n",
    "\n",
    "# 进行tokenization\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# 打印tokens及其对应的ID\n",
    "for token, token_id in zip(tokens, token_ids):\n",
    "    print(f\"Token: {token}, Token ID: {token_id}\")\n",
    "target_char = \":\"\n",
    "target_token_id = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(target_char))\n",
    "print(tokenizer.tokenize(target_char), target_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if block_obj != 'block_spec':\n",
    "    block_by_heads=False\n",
    "    all_jaccard_scores=[]\n",
    "    random.seed(42) \n",
    "    for i, prompt in enumerate(dataset):\n",
    "        #print(\"prompt:\", prompt)\n",
    "        input_tokens = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        #print(\"model output:\",get_output_text_form_logits(model(**input_tokens).logits[:, -1]))\n",
    "        #with TraceDict(model, layers=attn_O+attn_Q+attn_V+attn_K, retain_input=True, retain_output=True) as _:\n",
    "        pred=model(**input_tokens).logits[:, -1]\n",
    "        clean_top_tks=torch.topk(pred, n_top_preds).indices.squeeze(0)\n",
    "\n",
    "        if block_by_heads:\n",
    "            jaccard_scores=torch.zeros(n_layers, n_heads).to(device)\n",
    "            spec_str+='_by_heads'\n",
    "            for l in range(n_layers):\n",
    "                print(f\"analyzing layer {l}...\")\n",
    "                for h in range(n_heads):\n",
    "                    block_fn = block_head(edit_layer=l, head_to_block=h)\n",
    "                    block_layers = [f'model.layers.{l}.self_attn.o_proj']\n",
    "                    with TraceDict(model, layers=block_layers, retain_input=False, retain_output=False, edit_output=block_fn) as _:\n",
    "                        pred=model(**input_tokens).logits[0, -1]\n",
    "                        jaccard_scores[l, h]= jaccard(clean_top_tks.tolist(), torch.topk(pred, n_top_preds).indices.tolist() )\n",
    "            all_jaccard_scores.append(jaccard_scores.unsqueeze(0))\n",
    "            save_dir=f'./visualization/loyalty'\n",
    "        else:\n",
    "            jaccard_scores=torch.zeros(n_layers).to(device)\n",
    "            #spec_str+='_by_layers'\n",
    "            for l in range(n_layers):\n",
    "                #print(f\"analyzing layer {l}...\")\n",
    "                block_fn = block_head(edit_layer=l, head_to_block=-1)\n",
    "                block_layers = [f'model.layers.{l}.self_attn.o_proj']\n",
    "                with TraceDict(model, layers=block_layers, retain_input=False, retain_output=False, edit_output=block_fn) as _:\n",
    "                    pred=model(**input_tokens).logits[0, -1]\n",
    "                    jaccard_scores[l]= jaccard(clean_top_tks.tolist(), torch.topk(pred, n_top_preds).indices.tolist() )\n",
    "                    \n",
    "            all_jaccard_scores.append(jaccard_scores.unsqueeze(0))\n",
    "            save_dir=f'./visualization/loyalty'\n",
    "                \n",
    "                \n",
    "    dataset_mean_loyalty=torch.mean(torch.cat(all_jaccard_scores,dim=0), dim=0)\n",
    "\n",
    "    if block_by_heads:\n",
    "        visualize_heatmap(data=dataset_mean_loyalty, title_name=task_name+spec_str, x_name='layers', y_name='loyalty')\n",
    "    else:\n",
    "        visualize_bar(data=dataset_mean_loyalty, title_name=task_name+spec_str, x_name='layers', y_name='loyalty')\n",
    "    spec_str+=f'_top{n_top_preds}tks_dataset-size-{dataset_size}_{n_context}-demos_block-last-tk'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_dir, task_name+spec_str+'.jpg'))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
